<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Anirudh Vemula</title>
    <link>https://vvanirudh.github.io/blog/</link>
    <description>Recent content in Blog on Anirudh Vemula</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Nov 2020 23:47:09 -0400</lastBuildDate><atom:link href="https://vvanirudh.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AdaGrad</title>
      <link>https://vvanirudh.github.io/blog/adagrad/</link>
      <pubDate>Wed, 18 Nov 2020 23:47:09 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/adagrad/</guid>
      <description>Francesco Orabona&amp;rsquo;s monograph on online learning has a very neat and intuitive way of looking at AdaGrad. I found it to be really useful.
Looking back at OGD From our note on online (sub)gradient descent, we had the following regret bound
$$R_T(u) \leq \frac{D^2}{2\eta_T} + \sum_{t=1}^T \frac{\eta_t}{2} ||g_t||_2^2 $$
where $D$ is the diameter of the closed, convex set $V$. If we assume a fixed learning rate for all time steps and optimizing we get the optimal learning rate as</description>
    </item>
    
    <item>
      <title>CMAX&#43;&#43;: Leveraging Experience for Planning and Execution with Inaccurate Models</title>
      <link>https://vvanirudh.github.io/blog/cmaxpp/</link>
      <pubDate>Tue, 13 Oct 2020 14:39:32 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/cmaxpp/</guid>
      <description>In this blog post, I will summarize the key contributions of my recent submission on CMAX++. This work mainly improves upon my previous work, CMAX (highly recommended to read a summary of CMAX in this blog post.)
The broad theme of both these approaches is to use simplified and inaccurate dynamical models to significantly reduce the amount of real world experience needed by the robot to complete a task. The setting we consider is a goal-oriented task setting where the robot needs to reach the goal online, without any resets.</description>
    </item>
    
    <item>
      <title>Regret Analysis of Stochastic Bandit Problems</title>
      <link>https://vvanirudh.github.io/blog/stochastic_bandits/</link>
      <pubDate>Thu, 23 Jul 2020 19:09:28 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/stochastic_bandits/</guid>
      <description>$K$ arms, $T$ rounds (assume known $K$ and $T$) Goal is to maximize total reward over $T$ rounds Assumptions Bandit Feedback: only observe reward for the arm played IID rewards: For each arm $a$, $\exists$ a reward distribution $D_a$ which is unknown Per-round rewards are bounded, i.e. $r_t \in [0, 1]$ for $t \in [T]$ We are primarily interested in mean vector $\mu \in [0, 1]^K$ where $\mu(a) = \mathbb{E}[D_a]$ Notation Set of arms $\mathcal{A}$ Best mean reward $\mu^* = \max_{a \in \mathcal{A}} \mu(a)$ Gap of arm $a$: $\Delta(a) = \mu^* - \mu(a)$ Optimal arm $a^* $, i.</description>
    </item>
    
    <item>
      <title>Online Learning for Adversaries with Past Memory</title>
      <link>https://vvanirudh.github.io/blog/ol_memory/</link>
      <pubDate>Thu, 09 Jul 2020 22:33:34 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/ol_memory/</guid>
      <description>The framework of OCO with memory is:
At each round $t$, player chooses $x_t \in K \subset \mathbb{R}^n$ Then, a loss $f_t: K^{m+1} \rightarrow \mathbb{R}$ is revealed and player suffers loss of $f_t(x_{t-m}, \cdots, x_t)$ Assume $0 \in K$, and $f_t(x_0, \cdots, x_m) \in [0,1]$ for any $x_0, \cdots, x_m \in K$ Assume that after $f_t$ is revealed, the player is aware of the loss she would suffer had she played any sequence of decisions $x_{t-m}, \cdots, x_t$ (counterfactual feedback model) Minimize policy regret $$R_{T, m} = \sum_{t=m}^T f_t(x_{t-m}, \cdots, x_t) - \min_{x \in K} \sum_{t=m}^T f_t(x, \cdots, x) $$</description>
    </item>
    
    <item>
      <title>Online Newton Step</title>
      <link>https://vvanirudh.github.io/blog/ons/</link>
      <pubDate>Sat, 04 Jul 2020 11:12:37 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/ons/</guid>
      <description>Exp-concave Functions OGD achieves logarithmic regret for strongly convex and lipschitz functions (see this note), but this is a rather strong condition.
For twice-differentiable strongly convex functions, we require that the hessian is positive definite and has full rank!
However, sometimes the hessian might not be full rank (sometimes, even rank one) but is large in the direction of the gradient. This is a property of exp-concave functions.
Exp-concave Definition: A convex function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is $\alpha$-exp-concave over $K \subseteq \mathbb{R}^n$ if the function $g$ is concave, where $g:K \rightarrow \mathbb{R}$ is $$ g(x) = \exp(-\alpha f(x)) $$</description>
    </item>
    
    <item>
      <title>Online Gradient Descent with Strongly Convex Functions</title>
      <link>https://vvanirudh.github.io/blog/strong_convexity/</link>
      <pubDate>Fri, 03 Jul 2020 16:52:01 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/strong_convexity/</guid>
      <description>Turns out we can do better $\sqrt{T}$ regret we saw for OGD analysis for convex lipschitz losses if they possess some curvature that can be exploited.
Strong Convexity $f$ is $\mu$-strongly convex over convex set $V$ w.r.t norm $||\cdot||$ if $\forall x, y \in V$ and $g \in \partial f(x)$ $$ f(y) \geq f(x) + \langle g, y - x\rangle +\frac{\mu}{2}||x - y||^2$$
In other words, a strongly convex function is lower bounded by a quadratic (instead of linear like convex functions).</description>
    </item>
    
    <item>
      <title>Online Gradient Descent</title>
      <link>https://vvanirudh.github.io/blog/ogd/</link>
      <pubDate>Wed, 01 Jul 2020 12:57:48 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/ogd/</guid>
      <description>Similar to SGD but different: loss functions are different at each time step
Algorithm Projected Online Gradient Descent: $x_1 \in V \subseteq \mathbb{R}^d$ where $V$ is a closed, convex set and $\eta_1, \cdots, \eta_T &amp;gt; 0$
for $t = 1, \cdots, T$ Play $x_t$ Receive $\ell_t: \mathbb{R}^d \rightarrow (-\infty, \infty]$ and pay $\ell_t(x_t)$ Set $g_t = \nabla \ell_t(x_t)$ $x_{t+1} = \Pi_V(x_t - \eta_t g_t) = \arg\min_{y \in V}||x_t - \eta_tg_t - y||_2$ end Regret Analysis Crucial to the analysis is the following simple lemma:</description>
    </item>
    
    <item>
      <title>Frank-Wolfe a.k.a Conditional Gradient Descent</title>
      <link>https://vvanirudh.github.io/blog/frank_wolfe/</link>
      <pubDate>Sat, 27 Jun 2020 22:15:13 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/frank_wolfe/</guid>
      <description>An alternative approach to projected gradient descent for minimizing a smooth convex function $f$ over a compact, convex set $K$.
At the $t$-th step, it considers the first-order taylor approximation $\hat{f}$ of $f$ around $x_t$, and minimizes $\hat{f}$ over $K$, obtaining a minimizer $y_t$. $$ \hat{f}(y) = f(x_t) + \langle \nabla f(x_t), y - x_t \rangle $$ $$ y_t = \arg\min_{y \in K} \hat{f}(y) = \arg\min_{y \in K} \langle \nabla f(x_t), y \rangle $$</description>
    </item>
    
    <item>
      <title>Forward and Reverse KL Divergence</title>
      <link>https://vvanirudh.github.io/blog/kl/</link>
      <pubDate>Fri, 26 Jun 2020 23:47:09 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/kl/</guid>
      <description>KL-divergence is given by: $$ D_{KL}(P||Q) = \mathbb{E}_{x \sim P}\left[\log \frac{P(X)}{Q(X)}\right] $$
It is not necessarily symmetric, so $D_{KL}(P||Q) \neq D_{KL}(Q||P)$, thus it cannot be used as a distance metric.
It is always positive, i.e. $D_{KL} \geq 0$. Also, if there is a point $x$ where $Q(X) = 0$ but $P(X) \neq 0$, then $D_{KL}(P||Q) = \infty$. Thus, we need the support of $P$ to lie completely in $Q$ for a finite KL-divergence.</description>
    </item>
    
    <item>
      <title>Hunter Thompson&#39;s letter on finding purpose</title>
      <link>https://vvanirudh.github.io/blog/letter/</link>
      <pubDate>Wed, 17 Jun 2020 14:39:32 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/letter/</guid>
      <description>Several years ago, I came across this letter written by Hunter S. Thompson to his friend Hume Logan on finding purpose and meaning in one&amp;rsquo;s life. It had a profound impact on me back then, and very recently I read the letter again only to interpret it differently. The advice in the letter is timeless and I like to think that reading it again in a few years will reveal other hidden gems in this letter.</description>
    </item>
    
    <item>
      <title>CMAX: Planning and Execution with Inaccurate Models</title>
      <link>https://vvanirudh.github.io/blog/cmax/</link>
      <pubDate>Wed, 06 May 2020 14:39:32 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/cmax/</guid>
      <description>Very recently, I submitted a paper to RSS (update: accepted!) on how to use inaccurate models for planning in robotic tasks. For many robotic tasks, we usually have access to a (inaccurate) model of the robot and the environment. For example, simulators that employ sophisticated physics engines such as MuJoCo or Bullet can be used as forward dynamical models for planning. These models capture some complexities of the real world well, while they fail to capture other subtleties.</description>
    </item>
    
    <item>
      <title>AISTATS 2019</title>
      <link>https://vvanirudh.github.io/blog/aistats19/</link>
      <pubDate>Fri, 19 Apr 2019 15:59:13 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/aistats19/</guid>
      <description>Thoughts on the conference My overall impression of AISTATS was that its a really good conference if statistical ML is what you are interested/working in. I learnt a fair bit about the basics of unknown topics and made a few friends who work in widely different areas. Its a single track conference so there&amp;rsquo;s more interaction among people and more exposure to the oral sessions. There&amp;rsquo;s barely any presence of industry (either in sponsors or papers) except for Google Research/Brain/Deepmind papers.</description>
    </item>
    
    <item>
      <title>New York City in 35mm</title>
      <link>https://vvanirudh.github.io/blog/newyork_pics/</link>
      <pubDate>Mon, 02 Oct 2017 15:59:13 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/newyork_pics/</guid>
      <description>Some of these photos were taken in New York city when I visited it in October for an AI forum. I wanted to take pictures in NYC for so long and felt this was as good a time as any. So, carried my old-ass camera around uptown and midtown manhattan, and took most of the photos.</description>
    </item>
    
    <item>
      <title>Graffiti in Pittsburgh through 35mm</title>
      <link>https://vvanirudh.github.io/blog/pittsburgh_graffiti/</link>
      <pubDate>Fri, 22 Sep 2017 15:59:13 -0400</pubDate>
      
      <guid>https://vvanirudh.github.io/blog/pittsburgh_graffiti/</guid>
      <description>Early fall 2017, I went on a search for graffiti in Pittsburgh and found a lot! Took a bunch of snaps and developed them on Ilford HP5 35mm. To get most of these I had to bike around Pittsburgh (mostly in Lawrenceville and Oakland). Never knew Pittsburgh had so much street art.</description>
    </item>
    
  </channel>
</rss>
